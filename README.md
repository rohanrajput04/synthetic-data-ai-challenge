# synthetic-data-ai-challenge

## ğŸ§  Overview
This repository presents a **comprehensive pipeline** for constructing and fine-tuning datasets aimed at **complex relational reasoning** tasks such as:

- ğŸ§¬ Family & Kinship reasoning  
- ğŸ©¸ Blood relation deduction  
- ğŸª‘ Seating arrangement (linear & circular)  
- ğŸ§© Multi-step logical reasoning  

We integrate multiple benchmark datasets, apply **prompt engineering and validation**, synthesize **high-difficulty Q&A pairs**, and fine-tune a large language model (LLM) using **supervised fine-tuning (SFT)**.

---

## ğŸ“š Dataset Sources

| Dataset | Focus Area | Reference |
|----------|-------------|------------|
| [RiddleBench](https://huggingface.co/datasets/ai4bharat/RiddleBench) | Logical riddles (includes â€œBlood Relationsâ€) | ai4bharat |
| [CLUTRR](https://github.com/facebookresearch/clutrr) | Family tree reasoning and relational inference | Facebook Research |
| [Kinship](https://github.com/juanshernandez/kinship) | Kinship relation triples for reasoning tasks | Open Source |

These were unified, normalized, and cleaned to form a **rich reasoning corpus**.

## âš™ï¸ Data Processing Pipeline

### ğŸ§© Step 1: Prompt Engineering & Validation
We employed multiple **prompt patterns** to increase linguistic variety and complexity:
- **Reformulation prompts** â†’ Diverse expression of relations  
- **Constraint-injection prompts** â†’ Add reasoning steps and ambiguity  
- **Self-consistency validation** â†’ Filter logically coherent samples  

âœ… *Result:* Highly challenging but solvable examples.

---

### ğŸ¤– Step 2: Synthetic Q&A Generation
We used Metaâ€™s [Synthetic Data Kit](https://github.com/meta-llama/synthetic-data-kit) to transform reasoning texts into questionâ€“answer pairs:

```bash
synthetic-data-kit -c configs/reasoning_config.yaml create ./data/parsed/ --type qa --num-pairs 50
