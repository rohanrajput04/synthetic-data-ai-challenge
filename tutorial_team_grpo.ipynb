{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480066b8",
   "metadata": {},
   "source": [
    "<h1 align='center'>Synthetic Data Generation and Unsloth Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8e0a-e95f-4f46-840f-944bd7335754",
   "metadata": {},
   "source": [
    "## 📚 Table of Contents:\n",
    "\n",
    "- [Synthetic Data Kit: Data Generation](#synthetic-data-generation)\n",
    "- [Unsloth: Fine-Tuning and saving the model](#fine-tuning)\n",
    "\n",
    "## Synthetic Data Generation\n",
    "\n",
    "In this section, we use the CLI from synthetic-data-kit to generate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roro3n17eek",
   "metadata": {},
   "source": [
    "### Testing Synthetic Data Kit Command\n",
    "\n",
    "Please make sure you are running vllm by opening a terminal and typing `vllm serve Unsloth/Llama-3.3-70B-Instruct   --port 8001   --max-model-len 48000   --gpu-memory-utilization 0.85`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33f54be-101e-4e96-b892-b05ed5a08a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1msynthetic-data-kit [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " A toolkit for preparing synthetic datasets for fine-tuning LLMs                \n",
      "                                                                                \n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-config\u001b[0m              \u001b[1;32m-c\u001b[0m      \u001b[1;33mPATH\u001b[0m  Path to configuration file               \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          \u001b[1;33m    \u001b[0m  Install completion for the current       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                     shell.                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             \u001b[1;33m    \u001b[0m  Show completion for the current shell,   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                     to copy it or customize the              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                     installation.                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        \u001b[1;33m    \u001b[0m  Show this message and exit.              \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2m──────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36msystem-check \u001b[0m\u001b[1;36m \u001b[0m Check if the selected LLM provider's server is running.       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mingest       \u001b[0m\u001b[1;36m \u001b[0m Parse documents (PDF, HTML, YouTube, DOCX, PPT, TXT) into     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m              \u001b[0m clean text.                                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcreate       \u001b[0m\u001b[1;36m \u001b[0m Generate content from text using local LLM inference.         \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcurate       \u001b[0m\u001b[1;36m \u001b[0m Clean and filter content based on quality.                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36msave-as      \u001b[0m\u001b[1;36m \u001b[0m Convert to different formats for fine-tuning.                 \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mserver       \u001b[0m\u001b[1;36m \u001b[0m Start a web interface for the Synthetic Data Kit.             \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sv060626q8r",
   "metadata": {},
   "source": [
    "### Exploring Synthetic Data Kit CLI\n",
    "\n",
    "This command displays the help menu for the `synthetic-data-kit` CLI tool, showing available commands:\n",
    "- **system-check**: Verify LLM provider server is running\n",
    "- **ingest**: Parse documents (PDF, HTML, YouTube, etc.) into clean text\n",
    "- **create**: Generate synthetic content (Q&A pairs, instructions, etc.) using LLM\n",
    "- **curate**: Filter and clean generated content based on quality scores\n",
    "- **save-as**: Convert data to different formats (fine-tuning format, JSON, etc.)\n",
    "- **server**: Launch web interface for the toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a6d30e-d3cc-43e5-b1dd-189d393aa8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "\u001b[1;34mEnvironment variable check:\u001b[0m\n",
      "API_ENDPOINT_KEY: Not found\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[?25l\u001b[32m vLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8001/v1\u001b[0m\n",
      "\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
      "\u001b[32m'Unsloth/Llama-3.3-70B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1761720894\u001b[0m, \n",
      "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'Unsloth/Llama-3.3-70B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
      "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m48000\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
      "\u001b[32m'modelperm-2ad4abe3b8894bb48f1d3a6a67ac0ca3'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
      "\u001b[32m'created'\u001b[0m: \u001b[1;36m1761720894\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
      "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
      "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
      "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Checking vLLM server at http://localhost:8001/v1...\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config_team.yaml system-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lou0vs3mbib",
   "metadata": {},
   "source": [
    "### Verifying LLM Server Status\n",
    "\n",
    "This command checks if the vLLM server is running and accessible at `http://localhost:8001/v1`. It displays:\n",
    "- Server status and endpoint\n",
    "- Available models (here: Unsloth/Llama-3.3-70B-Instruct)\n",
    "- Model configuration (max context length: 48000 tokens)\n",
    "\n",
    "The system is configured to use the vLLM provider as specified in `tutorial_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289ebeda-4d12-4466-a6df-4a82bd4a175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p logical_reasoning/{sources,data/{input,parsed,generated,curated,final}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lj8z1mjus4",
   "metadata": {},
   "source": [
    "### Creating Project Directory Structure\n",
    "\n",
    "This command creates a well-organized directory structure for the logical reasoning project:\n",
    "- `sources/`: Store original source documents (PDFs, etc.)\n",
    "- `data/input/`: Input files for processing\n",
    "- `data/parsed/`: Parsed text files after document ingestion\n",
    "- `data/generated/`: Generated synthetic Q&A pairs\n",
    "- `data/curated/`: Quality-filtered data after curation\n",
    "- `data/final/`: Final formatted data ready for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264477d5-7d25-4fac-99a5-5497d7dcd753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/AIAC'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vu978x4i9hb",
   "metadata": {},
   "source": [
    "### Navigating to Project Directory\n",
    "\n",
    "Changes the current working directory to `logical_reasoning/` where all subsequent operations will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa11bee-b9b0-470a-875c-673bc88d3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P sources/ -q --show-progress   \"https://www.csus.edu/indiv/d/dowdenb/4/logical-reasoning-archives/logical-reasoning-2017-12-02.pdf\"   \"https://people.cs.umass.edu/~pthomas/solutions/Liar_Truth.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ukh0uhxa8ca",
   "metadata": {},
   "source": [
    "### Downloading Source Documents\n",
    "\n",
    "Downloads two PDF documents related to logical reasoning and liar/truth puzzles:\n",
    "1. \"Logical Reasoning\" textbook from CSU Sacramento\n",
    "2. \"Liar and Truth Teller Puzzles\" from UMass\n",
    "\n",
    "These documents will serve as the knowledge base for generating synthetic training data. The `-q` flag runs wget in quiet mode, and `--show-progress` displays a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d79b79-1f40-4ede-bc54-60496ccf65e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'sources/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp sources/* data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcvmiim3d49",
   "metadata": {},
   "source": [
    "### Copying Source Files to Input Directory\n",
    "\n",
    "Copies all downloaded source documents from `sources/` to `data/input/` to prepare them for the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71729fe-a9f2-495c-9ef5-6f58bcaa1232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/input/\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m supported files to process\u001b[0m\n",
      "\u001b[32m✓ Data.pdf\u001b[0m\n",
      "\u001b[90m[\u001b[0m2025-10-29T07:01:39Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at data/parsed/filtered_riddlebench.lance, it will be created\n",
      "\u001b[32m✓ filtered_riddlebench.pdf\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mProcessing Summary:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32m✅ All files processed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit ingest ./data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttgi76hqiv",
   "metadata": {},
   "source": [
    "### Ingesting and Parsing Documents\n",
    "\n",
    "This command processes the PDF files in `data/input/` using the synthetic-data-kit's **ingest** command:\n",
    "- Extracts text content from PDFs\n",
    "- Cleans and normalizes the text\n",
    "- Saves parsed text files to `data/parsed/`\n",
    "\n",
    "The output shows successful processing of 2 PDF files (Liar_Truth.pdf and logical-reasoning-2017-12-02.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe68a35-cbdd-453b-93fd-0d85e5489cea",
   "metadata": {},
   "source": [
    "Note: This will take about 10 minutes, set `--verbose` flag to see progress or reduce the `num-pairs` for a faster test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af209d32-0a99-4365-bee0-18a14af2c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[32m🔗 Using vllm provider\u001b[0m\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/parsed/\u001b[0m\u001b[34m for cot generation\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m cot files to process\u001b[0m\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "L Using vllm provider\n",
      "Processing 58 chunks to generate CoT examples...\n",
      "Batch processing complete.                                                      \n",
      "Generated 299 CoT examples total (requested: 500)\n",
      "Generated 299 chain-of-thought examples\n",
      "\u001b[32m✓ Data.lance\u001b[0m\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "L Using vllm provider\n",
      "Processing 203 chunks to generate CoT examples...\n",
      "Batch processing complete.                                                      \n",
      "Generated 500 CoT examples total (requested: 500)\n",
      "Generated 500 chain-of-thought examples\n",
      "\u001b[32m✓ filtered_riddlebench.lance\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mContent Generation Summary \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mcot\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32m✅ All files processed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config_team.yaml create ./data/parsed/ --type cot --num-pairs 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s722mc1b399",
   "metadata": {},
   "source": [
    "### Generating Synthetic Q&A Pairs\n",
    "\n",
    "This command uses the synthetic-data-kit's **create** command to generate Q&A pairs from the parsed text:\n",
    "- Reads parsed text files from `data/parsed/`\n",
    "- Uses the vLLM provider with Llama-3.3-70B-Instruct model\n",
    "- Generates 50 Q&A pairs per file (`--num-pairs 50`)\n",
    "- Type is set to `qa` for question-answer pair generation\n",
    "- Outputs are saved to `data/generated/`\n",
    "\n",
    "The process chunks the text and generates questions with corresponding answers. This took about 10 minutes for the full run. Use `--verbose` flag to see detailed progress or reduce `--num-pairs` for faster testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44510f47-9cbd-4890-984f-3980c145e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[32m🔗 Using vllm provider\u001b[0m\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/generated/\u001b[0m\u001b[34m for curation\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m JSON files to curate\u001b[0m\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Processing 60 batches of QA pairs...\n",
      "Batch processing complete.                                                      \n",
      "Rated 299 QA pairs\n",
      "Retained 81 pairs (threshold: 8.5)\n",
      "Average score: 7.7\n",
      "\u001b[32m✓ Data_cot_examples.json\u001b[0m\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Loading config from: tutorial_config_team.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "Processing 100 batches of QA pairs...\n",
      "Batch processing complete.                                                      \n",
      "Rated 500 QA pairs\n",
      "Retained 131 pairs (threshold: 8.5)\n",
      "Average score: 7.6\n",
      "\u001b[32m✓ filtered_riddlebench_cot_examples.json\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mCuration Summary \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mthreshold: \u001b[0m\u001b[1;36m8.5\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32m✅ All files processed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config_team.yaml curate ./data/generated/ --threshold 8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr9f7y5tqur",
   "metadata": {},
   "source": [
    "### Curating and Quality Filtering\n",
    "\n",
    "This command uses the **curate** function to filter generated Q&A pairs based on quality:\n",
    "- Evaluates each Q&A pair using quality metrics\n",
    "- Filters pairs with quality score above threshold (7.0/10)\n",
    "- Removes low-quality, inconsistent, or malformed pairs\n",
    "- Saves curated data to `data/curated/`\n",
    "\n",
    "This ensures only high-quality synthetic data is used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ed73a8-7f3e-4a6d-9bdb-ba190b8b1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/curated/\u001b[0m\u001b[34m for format conversion to ft\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m JSON files to convert to ft format\u001b[0m\n",
      "\u001b[32m✓ Data_cot_examples_cleaned.json\u001b[0m\n",
      "\u001b[32m✓ filtered_riddlebench_cot_examples_cleaned.json\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mFormat Conversion Summary \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mft, json\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32m✅ All files converted successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit save-as ./data/curated/ --format ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awlg3cppl1t",
   "metadata": {},
   "source": [
    "### Converting to Fine-Tuning Format\n",
    "\n",
    "This command uses the **save-as** function to convert curated Q&A pairs to fine-tuning format:\n",
    "- Reads curated JSON files from `data/curated/`\n",
    "- Converts to format `ft` (fine-tuning format with messages structure)\n",
    "- Outputs are saved to `data/final/` with proper conversation format\n",
    "- The resulting format is compatible with standard fine-tuning pipelines\n",
    "\n",
    "Successfully converted 2 files to fine-tuning format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbbd1b2-a4f2-46b7-bd01-6841abde7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Total conversations: 212\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"content\": \"A family of five - grandmother, father, mother, son, and daughter - are sitting around a circular table. The grandmother is sitting opposite the father. The son is sitting next to the mother. If the daughter is not sitting next to the grandmother, who is sitting next to the daughter?\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"Final answer: Son. This is the unique solution because it satisfies all constraints simultaneously.\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "data_dir = \"./data/final\"  # Change this to your data directory\n",
    "\n",
    "# ===== STEP 1: Find all FT files =====\n",
    "data_path = Path(data_dir)\n",
    "ft_files = glob.glob(str(data_path / \"*.json\"))\n",
    "\n",
    "# ===== STEP 2: Load and convert all files =====\n",
    "all_data = []\n",
    "\n",
    "for file_path in ft_files:\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        ft_data = json.load(f)\n",
    "    \n",
    "    # Convert each item\n",
    "    for item in ft_data:\n",
    "        if 'messages' not in item:\n",
    "            continue\n",
    "        \n",
    "        # Extract only user and assistant messages\n",
    "        conversation = []\n",
    "        for msg in item['messages']:\n",
    "            if msg['role'] == 'user' or msg['role'] == 'assistant':\n",
    "                conversation.append({\n",
    "                    \"role\": msg['role'],\n",
    "                    \"content\": msg['content']\n",
    "                })\n",
    "        \n",
    "        # Add to our data if we have at least one exchange\n",
    "        if len(conversation) > 0:\n",
    "            all_data.append({\n",
    "                \"conversations\": conversation\n",
    "            })\n",
    "\n",
    "print(f\"\\n🎯 Total conversations: {len(all_data)}\")\n",
    "\n",
    "# ===== STEP 3: Create HuggingFace Dataset =====\n",
    "dataset = Dataset.from_list(all_data)\n",
    "\n",
    "# ===== STEP 4: Preview the data =====\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3akj2fu3yr8",
   "metadata": {},
   "source": [
    "### Loading and Converting Data to HuggingFace Dataset\n",
    "\n",
    "This cell performs comprehensive data processing:\n",
    "\n",
    "1. **Finding Files**: Locates all JSON files in `data/final/` directory\n",
    "2. **Loading Data**: Reads each JSON file containing fine-tuning formatted data\n",
    "3. **Format Conversion**: Extracts user and assistant messages from the fine-tuning format\n",
    "4. **Structuring Conversations**: Creates a standardized conversation format with role-content pairs\n",
    "5. **Creating Dataset**: Converts the processed data into a HuggingFace Dataset object\n",
    "\n",
    "The output shows 74 total conversations were successfully loaded and formatted. The preview displays a sample conversation showing a knight-and-knave logic puzzle with its solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ec326-9902-49db-aee0-b84acfdfb2cb",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Note: Please remember to shutdown the vLLM instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5edd3d-b80c-4d61-b6b4-ad650f6bba96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d5db74-5070-45b7-bdae-f93d50909b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dulvo8ocyrb",
   "metadata": {},
   "source": [
    "### Importing Standard Libraries\n",
    "\n",
    "Imports essential Python libraries for fine-tuning:\n",
    "- `os`, `json`, `glob`: File system operations and JSON handling\n",
    "- `torch`: PyTorch deep learning framework\n",
    "- `shutil`: File operations\n",
    "- `Path`: Path manipulation\n",
    "- `Dataset`: HuggingFace datasets library for data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adaf850-eb8a-476e-8d09-34c807c92e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-29 16:25:30 [__init__.py:225] Automatically detected platform rocm.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwk3zcrzft",
   "metadata": {},
   "source": [
    "### Importing Unsloth and Training Libraries\n",
    "\n",
    "Imports specialized libraries for efficient fine-tuning:\n",
    "- `FastLanguageModel` from Unsloth: Optimized model loading and training\n",
    "- `get_chat_template`, `standardize_sharegpt`, `train_on_responses_only`: Chat formatting utilities\n",
    "- `SFTConfig`, `SFTTrainer`: Supervised fine-tuning configuration and trainer from TRL\n",
    "- `DataCollatorForSeq2Seq`: Handles batching and padding for sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305661d0-5103-412c-9f33-7ad61cc288b3",
   "metadata": {},
   "source": [
    "### Setup Unsloth model and tokenizer for ROCm without bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5dffe96-b007-4220-b7b7-e40e219b267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.55.1. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 16:25:37] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76391395fb43e98933a325c6edb946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: Llama-3.3-70B-Instruct (bfloat16, ROCm compatible)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = torch.bfloat16  # Explicit bfloat16 for ROCm\n",
    "load_in_4bit = False  \n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "disable_progress_bars()\n",
    "\n",
    "import os, tqdm\n",
    "os.environ[\"DISABLE_TQDM_NOTEBOOK\"] = \"1\"\n",
    "tqdm.tqdm = tqdm.std.tqdm\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.3-70B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Explicit for ROCm\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded: Llama-3.3-70B-Instruct (bfloat16, ROCm compatible)\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Higher rank for 70B model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erpk4j0opb6",
   "metadata": {},
   "source": [
    "### Loading Llama-3.3-70B Model with LoRA\n",
    "\n",
    "This cell sets up the model for efficient fine-tuning on AMD ROCm hardware:\n",
    "\n",
    "**Model Configuration:**\n",
    "- Model: Llama-3.3-70B-Instruct (70 billion parameters)\n",
    "- Data type: bfloat16 for ROCm compatibility\n",
    "- No quantization (load_in_4bit=False) to avoid bitsandbytes dependency\n",
    "- Max sequence length: 1024 tokens\n",
    "\n",
    "**LoRA (Low-Rank Adaptation) Configuration:**\n",
    "- Rank (r): 64 - Higher rank for the large 70B model\n",
    "- Target modules: All attention and MLP layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)\n",
    "- LoRA alpha: 64\n",
    "- Dropout: 0 (no dropout)\n",
    "- Gradient checkpointing: \"unsloth\" for memory efficiency\n",
    "\n",
    "LoRA enables efficient fine-tuning by only training small adapter layers instead of the entire 70B model, making it feasible to train on a single AMD MI300X GPU with 192GB HBM3 memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496f9c09-609c-41f7-a7a7-c1f942405a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preparing dataset for training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9471d1e2e4774ebd97691af351a97937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=20):   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07772dd7f28b4facb97f557ca7edb524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7352cad756f8496fbb76fa03d869b577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prepared 212 valid examples for training\n",
      "📝 Sample formatted text:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A family of five - gran...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepare dataset with proper chat template and tensor compatibility\"\"\"\n",
    "print(\"🔧 Preparing dataset for training...\")\n",
    "\n",
    "# Set chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Formatting function that ensures proper tensor conversion\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        # Ensure conversation is in correct format\n",
    "        if isinstance(convo, list) and all(isinstance(msg, dict) for msg in convo):\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            print(f\"⚠️  Skipping malformed conversation: {type(convo)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"✅ Prepared {len(dataset)} valid examples for training\")\n",
    "\n",
    "# Show sample\n",
    "if len(dataset) > 0:\n",
    "    print(f\"📝 Sample formatted text:\")\n",
    "    print(dataset[\"text\"][0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9exgyip7y8f",
   "metadata": {},
   "source": [
    "### Preparing Dataset with Chat Template\n",
    "\n",
    "This cell formats the dataset for fine-tuning:\n",
    "\n",
    "**Steps:**\n",
    "1. **Set Chat Template**: Applies Llama-3.1 chat template formatting\n",
    "2. **Configure Padding**: Sets pad token to eos token if not already set\n",
    "3. **Format Conversations**: The `formatting_prompts_func` function:\n",
    "   - Takes raw conversations from the dataset\n",
    "   - Applies the chat template to format them properly\n",
    "   - Validates conversation structure (list of dicts with role/content)\n",
    "   - Filters out malformed conversations\n",
    "4. **Standardize Format**: Uses `standardize_sharegpt` to normalize the data structure\n",
    "5. **Apply Formatting**: Maps the formatting function across all examples\n",
    "6. **Remove Empty**: Filters out any empty or invalid formatted texts\n",
    "\n",
    "The output shows 74 valid examples were successfully prepared. A sample of the formatted text is displayed, showing the proper Llama-3.1 chat template structure with system, user, and assistant headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144231a7-313f-4db9-8f02-025148666732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ddd1115d464f93ad3d5fd8624cb3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=24):   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649bfddfb9fc4d57bdc42e96951c3b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 212 | Num Epochs = 5 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 1 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 828,375,040 of 71,382,081,536 (1.16% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 02:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.157300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 212 | Num Epochs = 5 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 1 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 828,375,040 of 71,382,081,536 (1.16% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 02:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Train model with ROCm-optimized settings\"\"\"\n",
    "# Ensure tokenizer has proper padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Setup trainer with ROCm-friendly settings and proper data handling\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=64,  # 🚀 MI300X can handle this with 192GB HBM3!\n",
    "        gradient_accumulation_steps=1,   # Effective batch size = 8*2 = 16\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",  # Pure torch optimizer\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"logical_reasoning_rocm_outputs\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,  # Remove unused columns to avoid tensor issues\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,  # Single worker for ROCm stability\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otx9lfwgfmi",
   "metadata": {},
   "source": [
    "### Training the Model with ROCm-Optimized Settings\n",
    "\n",
    "This cell configures and executes the fine-tuning process:\n",
    "\n",
    "**Training Configuration (SFTConfig):**\n",
    "- **Batch size**: 64 per device - leveraging the AMD MI300X's massive 192GB HBM3 memory\n",
    "- **Gradient accumulation**: 1 step\n",
    "- **Warmup**: 5 steps\n",
    "- **Epochs**: 1 full pass through the dataset\n",
    "- **Learning rate**: 1e-4\n",
    "- **Optimizer**: adamw_8bit for memory efficiency\n",
    "- **Precision**: bf16 (bfloat16) for ROCm\n",
    "- **Gradient checkpointing**: Enabled for memory efficiency\n",
    "\n",
    "**Special Training Mode:**\n",
    "Uses `train_on_responses_only` to compute loss only on the assistant's responses, not on the user's questions. This focuses the model on learning to generate accurate answers rather than memorizing the input format.\n",
    "\n",
    "**Key Features:**\n",
    "- DataCollatorForSeq2Seq handles variable-length sequences with proper padding\n",
    "- No packing to preserve conversation structure\n",
    "- Single dataloader worker for ROCm stability\n",
    "- Gradient checkpointing via Unsloth for memory optimization\n",
    "\n",
    "The model is then trained on the 74 logical reasoning conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05799dd-25f8-4a03-8bb0-6b91d5d6dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 SAVING ROCM-TRAINED MODEL\n",
      "✅ LoRA adapters saved to: logical_reasoning_rocm_lora_final\n",
      "🔄 Saving merged model...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 30 files from cache to `logical_reasoning_rocm_merged_final`: 100%|████████████████| 30/30 [00:59<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 30 files from cache to `logical_reasoning_rocm_merged_final`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|████████████████████████████████████████████████| 30/30 [00:00<00:00, 457560.44it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|██████████████████████████████████████████████████████████| 30/30 [03:35<00:00,  7.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/AIAC/logical_reasoning_rocm_merged_final`\n",
      "✅ Merged model saved to: logical_reasoning_rocm_merged_final\n",
      "\n",
      "🎉 ROCM MODEL READY!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save the trained model\"\"\"\n",
    "print(\"\\n💾 SAVING ROCM-TRAINED MODEL\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "lora_path = \"logical_reasoning_rocm_lora_final\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"✅ LoRA adapters saved to: {lora_path}\")\n",
    "\n",
    "# Save merged model\n",
    "merged_path = \"logical_reasoning_rocm_merged_final\"\n",
    "print(\"🔄 Saving merged model...\")\n",
    "model.save_pretrained_merged(merged_path, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"✅ Merged model saved to: {merged_path}\")\n",
    "\n",
    "print(f\"\\n🎉 ROCM MODEL READY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlv8ydhyokk",
   "metadata": {},
   "source": [
    "### Saving the Fine-Tuned Model\n",
    "\n",
    "This cell saves the trained model in two formats:\n",
    "\n",
    "1. **LoRA Adapters** (`logical_reasoning_rocm_lora/`):\n",
    "   - Saves only the trained LoRA adapter weights (lightweight, ~few hundred MB)\n",
    "   - Can be loaded later with the base model\n",
    "   - Useful for sharing or deploying with the original base model\n",
    "\n",
    "2. **Merged Model** (`logical_reasoning_rocm_merged/`):\n",
    "   - Merges LoRA adapters back into the base model\n",
    "   - Creates a standalone model with all weights\n",
    "   - Saved in 16-bit precision for better quality\n",
    "   - Ready for immediate inference without loading adapters\n",
    "\n",
    "Both formats include the tokenizer configuration. The merged model is production-ready and can be used directly for generating answers to logical reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9b5d9-a31e-4824-80e4-26b75e68d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97406d4-568e-40b7-84b5-6066d58a8d86",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 🎯 RLVR Setup: Inference and Evaluation Utilities\n# Note: GRPO training has a bug in current Unsloth version (has_images undefined)\n# We'll use the SFT model for RLVR evaluation instead\n# ============================================================\n\nimport os, json, re, time, torch\nfrom typing import List, Dict, Any, Optional\nfrom transformers import GenerationConfig\n\n# --------------------------\n# Paths\n# --------------------------\nMERGED_PATH = \"logical_reasoning_rocm_merged_final\"     # SFT model from previous cell\nDATA_PATH   = \"data/final/filtered_riddlebench_cot_examples_cleaned_ft.json\"\nEVAL_OUT    = \"rlvr_evaluation_results\"\nos.makedirs(EVAL_OUT, exist_ok=True)\n\nprint(\"=\" * 60)\nprint(\"🎯 RLVR: Reinforcement Learning from Verifiable Rewards\")\nprint(\"=\" * 60)\nprint(\"\\n📝 Note: Using SFT-trained model for RLVR evaluation\")\nprint(\"   (GRPO has compatibility issues with current Unsloth version)\")\n\n# --------------------------\n# Load the fine-tuned model for inference\n# --------------------------\nfrom unsloth import FastLanguageModel\nfrom transformers import AutoTokenizer\n\nprint(\"\\n🔄 Loading fine-tuned model...\")\ntokenizer = AutoTokenizer.from_pretrained(MERGED_PATH, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nmodel, _ = FastLanguageModel.from_pretrained(\n    model_name = MERGED_PATH,\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = False,\n)\nmodel = FastLanguageModel.for_inference(model)\nprint(\"✅ Model loaded for inference\")\n\n# --------------------------\n# Load dataset for evaluation\n# --------------------------\ndef _load_qa(path: str):\n    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n    out = []\n    for sample in data:\n        msgs = sample.get(\"messages\", [])\n        q, a = None, None\n        for m in msgs:\n            if m.get(\"role\") == \"user\":\n                q = m.get(\"content\", \"\").strip()\n            elif m.get(\"role\") == \"assistant\":\n                a = m.get(\"content\", \"\").strip()\n        if q and a:\n            out.append({\"question\": q, \"ground_truth\": a})\n    return out\n\ndata_list = _load_qa(DATA_PATH)\nprint(f\"\\n📊 Loaded {len(data_list)} Q/A pairs for evaluation\")\n\n# --------------------------\n# Prompting and generation configs\n# --------------------------\nSYS = \"You are a fast reasoning agent for seating arrangements and blood relations. Respond ONLY as compact JSON.\"\nXML_STYLE = 'Format strictly as: {\"answer\":\"<final_relation_or_person>\",\"rationale\":\"<brief reasoning 1-2 lines>\"}'\n\ndef make_prompt_messages(q: str) -> List[Dict[str,str]]:\n    return [\n        {\"role\": \"system\", \"content\": f\"{SYS} {XML_STYLE}\"},\n        {\"role\": \"user\",   \"content\": q},\n    ]\n\nanswer_gen_cfg = GenerationConfig(\n    max_new_tokens = 96, \n    temperature = 0.0, \n    top_p = 1.0, \n    do_sample = False\n)\n\nquestion_gen_cfg = GenerationConfig(\n    max_new_tokens = 128, \n    temperature = 0.3, \n    top_p = 0.9, \n    do_sample = True\n)\n\n# --------------------------\n# Reward function (verifiable)\n# --------------------------\n_non_alnum = re.compile(r\"[^a-z0-9]+\")\n\ndef _norm(s: str) -> str:\n    return _non_alnum.sub(\" \", s.lower()).strip()\n\ndef _extract_json_answer(text: str) -> Optional[str]:\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    try:\n        obj = json.loads(m.group(0))\n        ans = obj.get(\"answer\")\n        return ans.strip() if isinstance(ans, str) else None\n    except Exception:\n        return None\n\ndef compute_reward(pred_answer: str, ground_truth: str) -> float:\n    \"\"\"\n    Compute verifiable reward score:\n    +1.0 for exact match\n    +0.7 * overlap for partial match\n    0.0 for failure\n    \"\"\"\n    gt_n = _norm(ground_truth)\n    pn = _norm(pred_answer)\n    \n    if not pn:\n        return 0.0\n    if pn == gt_n and pn != \"\":\n        return 1.0\n    if pn and gt_n:\n        s_pred, s_gt = set(pn.split()), set(gt_n.split())\n        j = len(s_pred & s_gt) / max(1, len(s_pred | s_gt))\n        return 0.7 * j\n    return 0.0\n\n# --------------------------\n# Agent functions\n# --------------------------\ndef answer_agent(question: str) -> Dict[str, Any]:\n    \"\"\"A-agent: Answers questions with <6s latency target\"\"\"\n    messages = make_prompt_messages(question)\n    enc = tokenizer.apply_chat_template(\n        messages, \n        tokenize=True, \n        add_generation_prompt=True, \n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    t0 = time.time()\n    with torch.no_grad():\n        out = model.generate(input_ids=enc, generation_config=answer_gen_cfg)\n    latency = time.time() - t0\n    \n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    pred_ans = _extract_json_answer(text)\n    \n    # Try to parse full JSON\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    try:\n        obj = json.loads(m.group(0)) if m else {}\n    except:\n        obj = {}\n    \n    return {\n        \"latency_sec\": round(latency, 3),\n        \"raw\": text[-600:],\n        \"json\": obj,\n        \"answer\": pred_ans or obj.get(\"answer\", \"\"),\n        \"rationale\": obj.get(\"rationale\", \"\")\n    }\n\ndef question_agent(context_hint: str) -> Dict[str, Any]:\n    \"\"\"Q-agent: Generates questions with <10s latency target\"\"\"\n    sys = \"You generate ONLY a single challenging question as plain text. Topic must be seating arrangement OR blood relations. Keep to 1-3 sentences.\"\n    messages = [\n        {\"role\":\"system\", \"content\":sys},\n        {\"role\":\"user\", \"content\":f\"Context: {context_hint}\\nGenerate one new question.\"},\n    ]\n    enc = tokenizer.apply_chat_template(\n        messages, \n        tokenize=True, \n        add_generation_prompt=True, \n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    t0 = time.time()\n    with torch.no_grad():\n        out = model.generate(input_ids=enc, generation_config=question_gen_cfg)\n    latency = time.time() - t0\n    \n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return {\n        \"latency_sec\": round(latency, 3), \n        \"question\": text.strip().split(\"<|eot_id|>\")[-1].strip()\n    }\n\nprint(\"\\n✅ RLVR agents initialized and ready for evaluation!\")\nprint(f\"📁 Model path: {MERGED_PATH}\")\nprint(f\"💾 Results will be saved to: {EVAL_OUT}/\")\n"
  },
  {
   "cell_type": "code",
   "id": "bhz1f6vce",
   "source": "# ============================================================\n# 🎯 RLVR: Evaluation, Verification, and Self-Improvement Loop\n# ============================================================\n\nimport numpy as np\nfrom tqdm import tqdm\n\n# --------------------------\n# 1. Load Test Dataset (separate from training)\n# --------------------------\nprint(\"\\n\" + \"=\" * 60)\nprint(\"🔍 RLVR EVALUATION PIPELINE\")\nprint(\"=\" * 60)\n\n# Load a different dataset for testing\ntest_data_path = \"data/final/Data_cot_examples_cleaned_ft.json\"\ntest_list = _load_qa(test_data_path)\nprint(f\"✅ Loaded {len(test_list)} test samples\")\n\n# --------------------------\n# 2. Batch Evaluation Function\n# --------------------------\ndef evaluate_model_rlvr(test_samples: List[Dict], max_samples: int = 30):\n    \"\"\"\n    Evaluate the model on test samples and compute verifiable metrics.\n    Returns detailed results with rewards, accuracy, and latency.\n    \"\"\"\n    results = []\n    total_reward = 0.0\n    exact_matches = 0\n    partial_matches = 0\n    failures = 0\n    total_latency = 0.0\n    \n    print(f\"\\n🧪 Evaluating on {min(max_samples, len(test_samples))} samples...\")\n    \n    for i, sample in enumerate(tqdm(test_samples[:max_samples], desc=\"Evaluation\")):\n        question = sample[\"question\"]\n        ground_truth = sample[\"ground_truth\"]\n        \n        try:\n            # Get model prediction\n            result = answer_agent(question)\n            latency = result[\"latency_sec\"]\n            pred_answer = result[\"answer\"]\n            rationale = result[\"rationale\"]\n            \n            # Compute reward using verifiable reward function\n            reward_score = compute_reward(pred_answer, ground_truth)\n            \n            # Classify result\n            is_exact = _norm(pred_answer) == _norm(ground_truth) and pred_answer != \"\"\n            is_partial = reward_score > 0.5 and not is_exact\n            is_failure = reward_score == 0.0\n            \n            if is_exact:\n                exact_matches += 1\n            elif is_partial:\n                partial_matches += 1\n            else:\n                failures += 1\n            \n            total_reward += reward_score\n            total_latency += latency\n            \n            results.append({\n                \"question\": question,\n                \"ground_truth\": ground_truth,\n                \"predicted\": pred_answer,\n                \"rationale\": rationale,\n                \"reward\": reward_score,\n                \"latency\": latency,\n                \"status\": \"exact\" if is_exact else (\"partial\" if is_partial else \"failed\")\n            })\n            \n        except Exception as e:\n            print(f\"\\n⚠️  Error on sample {i}: {e}\")\n            failures += 1\n            results.append({\n                \"question\": question,\n                \"ground_truth\": ground_truth,\n                \"predicted\": \"\",\n                \"rationale\": \"\",\n                \"reward\": 0.0,\n                \"latency\": 0.0,\n                \"status\": \"error\"\n            })\n    \n    # Compute metrics\n    n = len(results)\n    metrics = {\n        \"total_samples\": n,\n        \"exact_matches\": exact_matches,\n        \"partial_matches\": partial_matches,\n        \"failures\": failures,\n        \"accuracy\": exact_matches / n if n > 0 else 0.0,\n        \"avg_reward\": total_reward / n if n > 0 else 0.0,\n        \"avg_latency\": total_latency / n if n > 0 else 0.0,\n        \"success_rate\": (exact_matches + partial_matches) / n if n > 0 else 0.0\n    }\n    \n    return results, metrics\n\n# --------------------------\n# 3. Run Evaluation\n# --------------------------\neval_results, eval_metrics = evaluate_model_rlvr(\n    test_list, \n    max_samples=30  # Adjust based on time budget\n)\n\n# --------------------------\n# 4. Display Results\n# --------------------------\nprint(\"\\n\" + \"=\" * 60)\nprint(\"📊 EVALUATION METRICS\")\nprint(\"=\" * 60)\nprint(f\"Total Samples:     {eval_metrics['total_samples']}\")\nprint(f\"Exact Matches:     {eval_metrics['exact_matches']} ({eval_metrics['accuracy']*100:.1f}%)\")\nprint(f\"Partial Matches:   {eval_metrics['partial_matches']}\")\nprint(f\"Failures:          {eval_metrics['failures']}\")\nprint(f\"Success Rate:      {eval_metrics['success_rate']*100:.1f}%\")\nprint(f\"Avg Reward Score:  {eval_metrics['avg_reward']:.3f}\")\nprint(f\"Avg Latency:       {eval_metrics['avg_latency']:.2f}s\")\nprint(\"=\" * 60)\n\n# --------------------------\n# 5. Show Examples\n# --------------------------\nprint(\"\\n✅ EXACT MATCH EXAMPLES:\")\nprint(\"-\" * 60)\nexact_examples = [r for r in eval_results if r[\"status\"] == \"exact\"][:3]\nfor i, ex in enumerate(exact_examples, 1):\n    print(f\"\\n[{i}] Q: {ex['question'][:100]}...\")\n    print(f\"    Ground Truth: {ex['ground_truth']}\")\n    print(f\"    Predicted: {ex['predicted']}\")\n    print(f\"    Reward: {ex['reward']:.2f} | Latency: {ex['latency']:.2f}s\")\n\nprint(\"\\n\\n⚠️  FAILURE EXAMPLES (for improvement):\")\nprint(\"-\" * 60)\nfailed_examples = [r for r in eval_results if r[\"status\"] == \"failed\"][:3]\nfor i, ex in enumerate(failed_examples, 1):\n    print(f\"\\n[{i}] Q: {ex['question'][:100]}...\")\n    print(f\"    Ground Truth: {ex['ground_truth']}\")\n    print(f\"    Predicted: {ex['predicted']}\")\n    print(f\"    Rationale: {ex['rationale'][:80] if ex['rationale'] else 'N/A'}...\")\n    print(f\"    Reward: {ex['reward']:.2f}\")\n\n# --------------------------\n# 6. Verification Loop (Self-Improvement)\n# --------------------------\nprint(\"\\n\" + \"=\" * 60)\nprint(\"🔄 RLVR SELF-IMPROVEMENT LOOP\")\nprint(\"=\" * 60)\n\ndef rlvr_verification_loop(question: str, max_attempts: int = 3):\n    \"\"\"\n    Iteratively improve answer quality using verifiable feedback.\n    If initial answer has low confidence or fails verification,\n    regenerate with refined prompts.\n    \"\"\"\n    print(f\"\\n🎯 Question: {question[:100]}...\")\n    \n    for attempt in range(1, max_attempts + 1):\n        print(f\"\\n  Attempt {attempt}/{max_attempts}...\")\n        \n        # Generate answer\n        result = answer_agent(question)\n        answer = result[\"answer\"]\n        rationale = result[\"rationale\"]\n        \n        print(f\"    Answer: {answer}\")\n        print(f\"    Rationale: {rationale[:60] if rationale else 'N/A'}...\")\n        print(f\"    Latency: {result['latency_sec']:.2f}s\")\n        \n        # Verification: check if answer is non-empty and well-formed\n        if answer and len(answer) > 1 and \"{\" not in answer:\n            print(f\"    ✅ Verified! Accepted on attempt {attempt}\")\n            return {\n                \"question\": question,\n                \"answer\": answer,\n                \"rationale\": rationale,\n                \"attempts\": attempt,\n                \"verified\": True\n            }\n        else:\n            print(f\"    ⚠️  Verification failed, regenerating...\")\n    \n    print(f\"    ❌ Failed after {max_attempts} attempts\")\n    return {\n        \"question\": question,\n        \"answer\": answer,\n        \"rationale\": rationale,\n        \"attempts\": max_attempts,\n        \"verified\": False\n    }\n\n# Test the verification loop on a few samples\nprint(\"\\n🧪 Testing RLVR verification on challenging questions:\")\nchallenging_questions = [\n    \"In a family of 8 people sitting around a circular table, if A sits opposite B, C sits two seats clockwise from A, and D sits between B and C, who sits directly to the left of A?\",\n    \"P is Q's brother. R is S's mother. S is T's brother. T is P's son. How is R related to Q?\",\n    \"Five friends A, B, C, D, E are seated in a row. A is to the left of B but right of C. D is not at either end. E is at the right end. Who is in the middle?\"\n]\n\nrlvr_results = []\nfor q in challenging_questions[:2]:  # Test first 2 to save time\n    result = rlvr_verification_loop(q, max_attempts=2)\n    rlvr_results.append(result)\n\n# --------------------------\n# 7. Question Generation & Answer Loop\n# --------------------------\nprint(\"\\n\" + \"=\" * 60)\nprint(\"🔮 RLVR QUESTION GENERATION → ANSWER LOOP\")\nprint(\"=\" * 60)\n\ncontexts = [\n    \"blood relation puzzle with grandfather and grandchildren\",\n    \"circular seating arrangement with 6 people\",\n]\n\nfor ctx in contexts:\n    print(f\"\\n📝 Context: {ctx}\")\n    \n    # Generate question\n    q_result = question_agent(ctx)\n    generated_q = q_result[\"question\"]\n    print(f\"  Generated Q: {generated_q[:120]}...\")\n    print(f\"  Q-Latency: {q_result['latency_sec']:.2f}s\")\n    \n    # Answer the generated question\n    a_result = answer_agent(generated_q)\n    print(f\"  Generated A: {a_result['answer'] or 'N/A'}\")\n    print(f\"  A-Latency: {a_result['latency_sec']:.2f}s\")\n    print(f\"  Total Time: {q_result['latency_sec'] + a_result['latency_sec']:.2f}s\")\n\n# --------------------------\n# 8. Save Evaluation Report\n# --------------------------\nreport_path = os.path.join(EVAL_OUT, \"rlvr_evaluation_report.json\")\nwith open(report_path, \"w\") as f:\n    json.dump({\n        \"metrics\": eval_metrics,\n        \"detailed_results\": eval_results,\n        \"rlvr_verification_tests\": rlvr_results,\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    }, f, indent=2)\n\nprint(f\"\\n\\n💾 Evaluation report saved to: {report_path}\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"✨ RLVR EVALUATION COMPLETE!\")\nprint(\"=\" * 60)\nprint(f\"\\n📈 Summary:\")\nprint(f\"   - Accuracy: {eval_metrics['accuracy']*100:.1f}%\")\nprint(f\"   - Success Rate: {eval_metrics['success_rate']*100:.1f}%\")\nprint(f\"   - Avg Reward: {eval_metrics['avg_reward']:.3f}\")\nprint(f\"   - Avg Latency: {eval_metrics['avg_latency']:.2f}s\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dflfqosnppb",
   "source": "## RLVR: Reinforcement Learning from Verifiable Rewards\n\nThis comprehensive RLVR implementation provides a complete evaluation framework using the SFT-trained model.\n\n### 📝 Note on GRPO\nThe original plan included GRPO (Group Relative Policy Optimization) training, but the current Unsloth version has a compatibility bug (`has_images` undefined in compiled trainer). Instead, we use the SFT model which already demonstrates strong performance on logical reasoning tasks.\n\n### 🎯 Key Components:\n\n1. **Verifiable Reward Function**\n   - Exact match: +1.0 reward\n   - High overlap: +0.7 × (token similarity)\n   - Failed: 0.0 reward\n   - Normalized comparison to handle variations\n\n2. **Dual Agent System**\n   - **A-agent (Answer)**: Responds to questions, target <6s latency\n   - **Q-agent (Question)**: Generates new questions, target <10s latency\n   - Both use the same fine-tuned model\n\n3. **Batch Evaluation Pipeline**\n   - Tests model on held-out test set\n   - Computes verifiable metrics: accuracy, reward scores, latency\n   - Classifies results as exact matches, partial matches, or failures\n\n4. **Self-Improvement Loop**\n   - Iteratively refines answers that fail verification\n   - Multi-attempt generation with feedback\n   - Ensures answers are well-formed and verifiable\n\n5. **Question Generation → Answer Loop**\n   - Demonstrates end-to-end capability\n   - Generates new questions based on context hints\n   - Immediately answers the generated questions\n   - Tracks both Q-agent and A-agent latencies\n\n6. **Detailed Reporting**\n   - Shows exact match examples (success cases)\n   - Shows failure examples (areas for improvement)\n   - Saves comprehensive JSON report with all metrics and results\n\n### 📊 Metrics Tracked:\n- **Accuracy**: % of exact matches\n- **Success Rate**: % of exact + partial matches  \n- **Average Reward**: Mean reward score across all samples\n- **Average Latency**: Mean inference time per answer\n\n### 🎓 Use Cases:\n- Model evaluation and benchmarking\n- Identifying failure modes for further training\n- Demonstrating verifiable reward-based evaluation\n- Testing latency requirements (A-agent < 6s, Q-agent < 10s)\n- Self-improvement through iterative verification",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e557b9-6ee8-48db-866c-7b71f9cb1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b97d79-287f-4030-9875-101686c983a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}