{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480066b8",
   "metadata": {},
   "source": [
    "<h1 align='center'>Synthetic Data Generation and Unsloth Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8e0a-e95f-4f46-840f-944bd7335754",
   "metadata": {},
   "source": [
    "## 📚 Table of Contents:\n",
    "\n",
    "- [Synthetic Data Kit: Data Generation](#synthetic-data-generation)\n",
    "- [Unsloth: Fine-Tuning and saving the model](#fine-tuning)\n",
    "\n",
    "## Synthetic Data Generation\n",
    "\n",
    "In this section, we use the CLI from synthetic-data-kit to generate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roro3n17eek",
   "metadata": {},
   "source": [
    "### Testing Synthetic Data Kit Command\n",
    "\n",
    "Please make sure you are running vllm by opening a terminal and typing `vllm serve Unsloth/Llama-3.3-70B-Instruct   --port 8001   --max-model-len 48000   --gpu-memory-utilization 0.85`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33f54be-101e-4e96-b892-b05ed5a08a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1msynthetic-data-kit [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " A toolkit for preparing synthetic datasets for fine-tuning LLMs                \n",
      "                                                                                \n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-config\u001b[0m              \u001b[1;32m-c\u001b[0m      \u001b[1;33mPATH\u001b[0m  Path to configuration file               \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          \u001b[1;33m    \u001b[0m  Install completion for the current       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                     shell.                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             \u001b[1;33m    \u001b[0m  Show completion for the current shell,   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                     to copy it or customize the              \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                     installation.                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        \u001b[1;33m    \u001b[0m  Show this message and exit.              \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2m──────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36msystem-check \u001b[0m\u001b[1;36m \u001b[0m Check if the selected LLM provider's server is running.       \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mingest       \u001b[0m\u001b[1;36m \u001b[0m Parse documents (PDF, HTML, YouTube, DOCX, PPT, TXT) into     \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36m              \u001b[0m clean text.                                                   \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcreate       \u001b[0m\u001b[1;36m \u001b[0m Generate content from text using local LLM inference.         \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mcurate       \u001b[0m\u001b[1;36m \u001b[0m Clean and filter content based on quality.                    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36msave-as      \u001b[0m\u001b[1;36m \u001b[0m Convert to different formats for fine-tuning.                 \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m \u001b[1;36mserver       \u001b[0m\u001b[1;36m \u001b[0m Start a web interface for the Synthetic Data Kit.             \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sv060626q8r",
   "metadata": {},
   "source": [
    "### Exploring Synthetic Data Kit CLI\n",
    "\n",
    "This command displays the help menu for the `synthetic-data-kit` CLI tool, showing available commands:\n",
    "- **system-check**: Verify LLM provider server is running\n",
    "- **ingest**: Parse documents (PDF, HTML, YouTube, etc.) into clean text\n",
    "- **create**: Generate synthetic content (Q&A pairs, instructions, etc.) using LLM\n",
    "- **curate**: Filter and clean generated content based on quality scores\n",
    "- **save-as**: Convert data to different formats (fine-tuning format, JSON, etc.)\n",
    "- **server**: Launch web interface for the toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6d30e-d3cc-43e5-b1dd-189d393aa8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!synthetic-data-kit -c tutorial_config.yaml system-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lou0vs3mbib",
   "metadata": {},
   "source": [
    "### Verifying LLM Server Status\n",
    "\n",
    "This command checks if the vLLM server is running and accessible at `http://localhost:8001/v1`. It displays:\n",
    "- Server status and endpoint\n",
    "- Available models (here: Unsloth/Llama-3.3-70B-Instruct)\n",
    "- Model configuration (max context length: 48000 tokens)\n",
    "\n",
    "The system is configured to use the vLLM provider as specified in `tutorial_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ebeda-4d12-4466-a6df-4a82bd4a175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p logical_reasoning/{sources,data/{input,parsed,generated,curated,final}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lj8z1mjus4",
   "metadata": {},
   "source": [
    "### Creating Project Directory Structure\n",
    "\n",
    "This command creates a well-organized directory structure for the logical reasoning project:\n",
    "- `sources/`: Store original source documents (PDFs, etc.)\n",
    "- `data/input/`: Input files for processing\n",
    "- `data/parsed/`: Parsed text files after document ingestion\n",
    "- `data/generated/`: Generated synthetic Q&A pairs\n",
    "- `data/curated/`: Quality-filtered data after curation\n",
    "- `data/final/`: Final formatted data ready for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "264477d5-7d25-4fac-99a5-5497d7dcd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jpserver-1-open.html   kernel-57d534cf-e12f-4a45-ba73-7f31cb3d3557.json\n",
      "jpserver-1.json        kernel-ead20d46-70ba-48c9-b9eb-8cca4a82ed9c.json\n",
      "jupyter_cookie_secret\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a18c9f8-ab1d-4060-9797-5f456d22fc8d",
   "metadata": {},
   "source": [
    "### Navigating to Project Directory\n",
    "\n",
    "Changes the current working directory to `logical_reasoning/` where all subsequent operations will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa11bee-b9b0-470a-875c-673bc88d3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logical-reasoning-2 100%[===================>]   5.53M  1.47MB/s    in 4.4s    \n",
      "Liar_Truth.pdf      100%[===================>] 327.61K  --.-KB/s    in 0.1s    \n"
     ]
    }
   ],
   "source": [
    "!wget -P sources/ -q --show-progress   \"https://www.csus.edu/indiv/d/dowdenb/4/logical-reasoning-archives/logical-reasoning-2017-12-02.pdf\"   \"https://people.cs.umass.edu/~pthomas/solutions/Liar_Truth.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ukh0uhxa8ca",
   "metadata": {},
   "source": [
    "### Downloading Source Documents\n",
    "\n",
    "Downloads two PDF documents related to logical reasoning and liar/truth puzzles:\n",
    "1. \"Logical Reasoning\" textbook from CSU Sacramento\n",
    "2. \"Liar and Truth Teller Puzzles\" from UMass\n",
    "\n",
    "These documents will serve as the knowledge base for generating synthetic training data. The `-q` flag runs wget in quiet mode, and `--show-progress` displays a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d79b79-1f40-4ede-bc54-60496ccf65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp sources/* data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcvmiim3d49",
   "metadata": {},
   "source": [
    "### Copying Source Files to Input Directory\n",
    "\n",
    "Copies all downloaded source documents from `sources/` to `data/input/` to prepare them for the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71729fe-a9f2-495c-9ef5-6f58bcaa1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "!synthetic-data-kit ingest ./data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttgi76hqiv",
   "metadata": {},
   "source": [
    "### Ingesting and Parsing Documents\n",
    "\n",
    "This command processes the PDF files in `data/input/` using the synthetic-data-kit's **ingest** command:\n",
    "- Extracts text content from PDFs\n",
    "- Cleans and normalizes the text\n",
    "- Saves parsed text files to `data/parsed/`\n",
    "\n",
    "The output shows successful processing of 2 PDF files (Liar_Truth.pdf and logical-reasoning-2017-12-02.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe68a35-cbdd-453b-93fd-0d85e5489cea",
   "metadata": {},
   "source": [
    "Note: This will take about 10 minutes, set `--verbose` flag to see progress or reduce the `num-pairs` for a faster test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af209d32-0a99-4365-bee0-18a14af2c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!synthetic-data-kit -c ../tutorial_config.yaml create ./data/parsed/ --type qa --num-pairs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s722mc1b399",
   "metadata": {},
   "source": [
    "### Generating Synthetic Q&A Pairs\n",
    "\n",
    "This command uses the synthetic-data-kit's **create** command to generate Q&A pairs from the parsed text:\n",
    "- Reads parsed text files from `data/parsed/`\n",
    "- Uses the vLLM provider with Llama-3.3-70B-Instruct model\n",
    "- Generates 50 Q&A pairs per file (`--num-pairs 50`)\n",
    "- Type is set to `qa` for question-answer pair generation\n",
    "- Outputs are saved to `data/generated/`\n",
    "\n",
    "The process chunks the text and generates questions with corresponding answers. This took about 10 minutes for the full run. Use `--verbose` flag to see detailed progress or reduce `--num-pairs` for faster testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44510f47-9cbd-4890-984f-3980c145e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!synthetic-data-kit -c ../tutorial_config.yaml curate ./data/generated/ --threshold 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr9f7y5tqur",
   "metadata": {},
   "source": [
    "### Curating and Quality Filtering\n",
    "\n",
    "This command uses the **curate** function to filter generated Q&A pairs based on quality:\n",
    "- Evaluates each Q&A pair using quality metrics\n",
    "- Filters pairs with quality score above threshold (7.0/10)\n",
    "- Removes low-quality, inconsistent, or malformed pairs\n",
    "- Saves curated data to `data/curated/`\n",
    "\n",
    "This ensures only high-quality synthetic data is used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed73a8-7f3e-4a6d-9bdb-ba190b8b1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!synthetic-data-kit save-as ./data/curated/ --format ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awlg3cppl1t",
   "metadata": {},
   "source": [
    "### Converting to Fine-Tuning Format\n",
    "\n",
    "This command uses the **save-as** function to convert curated Q&A pairs to fine-tuning format:\n",
    "- Reads curated JSON files from `data/curated/`\n",
    "- Converts to format `ft` (fine-tuning format with messages structure)\n",
    "- Outputs are saved to `data/final/` with proper conversation format\n",
    "- The resulting format is compatible with standard fine-tuning pipelines\n",
    "\n",
    "Successfully converted 2 files to fine-tuning format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbd1b2-a4f2-46b7-bd01-6841abde7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "data_dir = \"./data/final\"  # Change this to your data directory\n",
    "\n",
    "# ===== STEP 1: Find all FT files =====\n",
    "data_path = Path(data_dir)\n",
    "ft_files = glob.glob(str(data_path / \"*.json\"))\n",
    "\n",
    "# ===== STEP 2: Load and convert all files =====\n",
    "all_data = []\n",
    "\n",
    "for file_path in ft_files:\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        ft_data = json.load(f)\n",
    "    \n",
    "    # Convert each item\n",
    "    for item in ft_data:\n",
    "        if 'messages' not in item:\n",
    "            continue\n",
    "        \n",
    "        # Extract only user and assistant messages\n",
    "        conversation = []\n",
    "        for msg in item['messages']:\n",
    "            if msg['role'] == 'user' or msg['role'] == 'assistant':\n",
    "                conversation.append({\n",
    "                    \"role\": msg['role'],\n",
    "                    \"content\": msg['content']\n",
    "                })\n",
    "        \n",
    "        # Add to our data if we have at least one exchange\n",
    "        if len(conversation) > 0:\n",
    "            all_data.append({\n",
    "                \"conversations\": conversation\n",
    "            })\n",
    "\n",
    "print(f\"\\n🎯 Total conversations: {len(all_data)}\")\n",
    "\n",
    "# ===== STEP 3: Create HuggingFace Dataset =====\n",
    "dataset = Dataset.from_list(all_data)\n",
    "\n",
    "# ===== STEP 4: Preview the data =====\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3akj2fu3yr8",
   "metadata": {},
   "source": [
    "### Loading and Converting Data to HuggingFace Dataset\n",
    "\n",
    "This cell performs comprehensive data processing:\n",
    "\n",
    "1. **Finding Files**: Locates all JSON files in `data/final/` directory\n",
    "2. **Loading Data**: Reads each JSON file containing fine-tuning formatted data\n",
    "3. **Format Conversion**: Extracts user and assistant messages from the fine-tuning format\n",
    "4. **Structuring Conversations**: Creates a standardized conversation format with role-content pairs\n",
    "5. **Creating Dataset**: Converts the processed data into a HuggingFace Dataset object\n",
    "\n",
    "The output shows 74 total conversations were successfully loaded and formatted. The preview displays a sample conversation showing a knight-and-knave logic puzzle with its solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ec326-9902-49db-aee0-b84acfdfb2cb",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Note: Please remember to shutdown the vLLM instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5edd3d-b80c-4d61-b6b4-ad650f6bba96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5db74-5070-45b7-bdae-f93d50909b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dulvo8ocyrb",
   "metadata": {},
   "source": [
    "### Importing Standard Libraries\n",
    "\n",
    "Imports essential Python libraries for fine-tuning:\n",
    "- `os`, `json`, `glob`: File system operations and JSON handling\n",
    "- `torch`: PyTorch deep learning framework\n",
    "- `shutil`: File operations\n",
    "- `Path`: Path manipulation\n",
    "- `Dataset`: HuggingFace datasets library for data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adaf850-eb8a-476e-8d09-34c807c92e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwk3zcrzft",
   "metadata": {},
   "source": [
    "### Importing Unsloth and Training Libraries\n",
    "\n",
    "Imports specialized libraries for efficient fine-tuning:\n",
    "- `FastLanguageModel` from Unsloth: Optimized model loading and training\n",
    "- `get_chat_template`, `standardize_sharegpt`, `train_on_responses_only`: Chat formatting utilities\n",
    "- `SFTConfig`, `SFTTrainer`: Supervised fine-tuning configuration and trainer from TRL\n",
    "- `DataCollatorForSeq2Seq`: Handles batching and padding for sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305661d0-5103-412c-9f33-7ad61cc288b3",
   "metadata": {},
   "source": [
    "### Setup Unsloth model and tokenizer for ROCm without bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dffe96-b007-4220-b7b7-e40e219b267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = torch.bfloat16  # Explicit bfloat16 for ROCm\n",
    "load_in_4bit = False  \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.3-70B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Explicit for ROCm\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded: Llama-3.3-70B-Instruct (bfloat16, ROCm compatible)\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Higher rank for 70B model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erpk4j0opb6",
   "metadata": {},
   "source": [
    "### Loading Llama-3.3-70B Model with LoRA\n",
    "\n",
    "This cell sets up the model for efficient fine-tuning on AMD ROCm hardware:\n",
    "\n",
    "**Model Configuration:**\n",
    "- Model: Llama-3.3-70B-Instruct (70 billion parameters)\n",
    "- Data type: bfloat16 for ROCm compatibility\n",
    "- No quantization (load_in_4bit=False) to avoid bitsandbytes dependency\n",
    "- Max sequence length: 1024 tokens\n",
    "\n",
    "**LoRA (Low-Rank Adaptation) Configuration:**\n",
    "- Rank (r): 64 - Higher rank for the large 70B model\n",
    "- Target modules: All attention and MLP layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)\n",
    "- LoRA alpha: 64\n",
    "- Dropout: 0 (no dropout)\n",
    "- Gradient checkpointing: \"unsloth\" for memory efficiency\n",
    "\n",
    "LoRA enables efficient fine-tuning by only training small adapter layers instead of the entire 70B model, making it feasible to train on a single AMD MI300X GPU with 192GB HBM3 memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f9c09-609c-41f7-a7a7-c1f942405a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare dataset with proper chat template and tensor compatibility\"\"\"\n",
    "print(\"🔧 Preparing dataset for training...\")\n",
    "\n",
    "# Set chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Formatting function that ensures proper tensor conversion\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        # Ensure conversation is in correct format\n",
    "        if isinstance(convo, list) and all(isinstance(msg, dict) for msg in convo):\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            print(f\"⚠️  Skipping malformed conversation: {type(convo)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"✅ Prepared {len(dataset)} valid examples for training\")\n",
    "\n",
    "# Show sample\n",
    "if len(dataset) > 0:\n",
    "    print(f\"📝 Sample formatted text:\")\n",
    "    print(dataset[\"text\"][0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9exgyip7y8f",
   "metadata": {},
   "source": [
    "### Preparing Dataset with Chat Template\n",
    "\n",
    "This cell formats the dataset for fine-tuning:\n",
    "\n",
    "**Steps:**\n",
    "1. **Set Chat Template**: Applies Llama-3.1 chat template formatting\n",
    "2. **Configure Padding**: Sets pad token to eos token if not already set\n",
    "3. **Format Conversations**: The `formatting_prompts_func` function:\n",
    "   - Takes raw conversations from the dataset\n",
    "   - Applies the chat template to format them properly\n",
    "   - Validates conversation structure (list of dicts with role/content)\n",
    "   - Filters out malformed conversations\n",
    "4. **Standardize Format**: Uses `standardize_sharegpt` to normalize the data structure\n",
    "5. **Apply Formatting**: Maps the formatting function across all examples\n",
    "6. **Remove Empty**: Filters out any empty or invalid formatted texts\n",
    "\n",
    "The output shows 74 valid examples were successfully prepared. A sample of the formatted text is displayed, showing the proper Llama-3.1 chat template structure with system, user, and assistant headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144231a7-313f-4db9-8f02-025148666732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train model with ROCm-optimized settings\"\"\"\n",
    "# Ensure tokenizer has proper padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Setup trainer with ROCm-friendly settings and proper data handling\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=64,  # 🚀 MI300X can handle this with 192GB HBM3!\n",
    "        gradient_accumulation_steps=1,   # Effective batch size = 8*2 = 16\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",  # Pure torch optimizer\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"logical_reasoning_rocm_outputs\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,  # Remove unused columns to avoid tensor issues\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,  # Single worker for ROCm stability\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otx9lfwgfmi",
   "metadata": {},
   "source": [
    "### Training the Model with ROCm-Optimized Settings\n",
    "\n",
    "This cell configures and executes the fine-tuning process:\n",
    "\n",
    "**Training Configuration (SFTConfig):**\n",
    "- **Batch size**: 64 per device - leveraging the AMD MI300X's massive 192GB HBM3 memory\n",
    "- **Gradient accumulation**: 1 step\n",
    "- **Warmup**: 5 steps\n",
    "- **Epochs**: 1 full pass through the dataset\n",
    "- **Learning rate**: 1e-4\n",
    "- **Optimizer**: adamw_8bit for memory efficiency\n",
    "- **Precision**: bf16 (bfloat16) for ROCm\n",
    "- **Gradient checkpointing**: Enabled for memory efficiency\n",
    "\n",
    "**Special Training Mode:**\n",
    "Uses `train_on_responses_only` to compute loss only on the assistant's responses, not on the user's questions. This focuses the model on learning to generate accurate answers rather than memorizing the input format.\n",
    "\n",
    "**Key Features:**\n",
    "- DataCollatorForSeq2Seq handles variable-length sequences with proper padding\n",
    "- No packing to preserve conversation structure\n",
    "- Single dataloader worker for ROCm stability\n",
    "- Gradient checkpointing via Unsloth for memory optimization\n",
    "\n",
    "The model is then trained on the 74 logical reasoning conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05799dd-25f8-4a03-8bb0-6b91d5d6dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save the trained model\"\"\"\n",
    "print(\"\\n💾 SAVING ROCM-TRAINED MODEL\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "lora_path = \"logical_reasoning_rocm_lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"✅ LoRA adapters saved to: {lora_path}\")\n",
    "\n",
    "# Save merged model\n",
    "merged_path = \"logical_reasoning_rocm_merged\"\n",
    "print(\"🔄 Saving merged model...\")\n",
    "model.save_pretrained_merged(merged_path, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"✅ Merged model saved to: {merged_path}\")\n",
    "\n",
    "print(f\"\\n🎉 ROCM MODEL READY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlv8ydhyokk",
   "metadata": {},
   "source": [
    "### Saving the Fine-Tuned Model\n",
    "\n",
    "This cell saves the trained model in two formats:\n",
    "\n",
    "1. **LoRA Adapters** (`logical_reasoning_rocm_lora/`):\n",
    "   - Saves only the trained LoRA adapter weights (lightweight, ~few hundred MB)\n",
    "   - Can be loaded later with the base model\n",
    "   - Useful for sharing or deploying with the original base model\n",
    "\n",
    "2. **Merged Model** (`logical_reasoning_rocm_merged/`):\n",
    "   - Merges LoRA adapters back into the base model\n",
    "   - Creates a standalone model with all weights\n",
    "   - Saved in 16-bit precision for better quality\n",
    "   - Ready for immediate inference without loading adapters\n",
    "\n",
    "Both formats include the tokenizer configuration. The merged model is production-ready and can be used directly for generating answers to logical reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9b5d9-a31e-4824-80e4-26b75e68d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97406d4-568e-40b7-84b5-6066d58a8d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
